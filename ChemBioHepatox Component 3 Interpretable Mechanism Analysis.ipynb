{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad418679-5431-492f-b4b4-490082456855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\syq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.60, Test Loss: 0.54, Test AUC: 0.79, Test Precision: 0.77\n",
      "Epoch 2/50, Train Loss: 0.51, Test Loss: 0.48, Test AUC: 0.84, Test Precision: 0.78\n",
      "Epoch 3/50, Train Loss: 0.45, Test Loss: 0.46, Test AUC: 0.86, Test Precision: 0.80\n",
      "Epoch 4/50, Train Loss: 0.39, Test Loss: 0.44, Test AUC: 0.87, Test Precision: 0.84\n",
      "Epoch 5/50, Train Loss: 0.34, Test Loss: 0.43, Test AUC: 0.88, Test Precision: 0.83\n",
      "Epoch 6/50, Train Loss: 0.31, Test Loss: 0.43, Test AUC: 0.89, Test Precision: 0.85\n",
      "Epoch 7/50, Train Loss: 0.28, Test Loss: 0.44, Test AUC: 0.89, Test Precision: 0.85\n",
      "Epoch 8/50, Train Loss: 0.24, Test Loss: 0.46, Test AUC: 0.89, Test Precision: 0.84\n",
      "Epoch 9/50, Train Loss: 0.23, Test Loss: 0.45, Test AUC: 0.90, Test Precision: 0.85\n",
      "Epoch 10/50, Train Loss: 0.21, Test Loss: 0.48, Test AUC: 0.90, Test Precision: 0.86\n",
      "Epoch 11/50, Train Loss: 0.19, Test Loss: 0.52, Test AUC: 0.90, Test Precision: 0.88\n",
      "Epoch 12/50, Train Loss: 0.18, Test Loss: 0.48, Test AUC: 0.91, Test Precision: 0.84\n",
      "Epoch 13/50, Train Loss: 0.16, Test Loss: 0.50, Test AUC: 0.91, Test Precision: 0.85\n",
      "Epoch 14/50, Train Loss: 0.15, Test Loss: 0.53, Test AUC: 0.90, Test Precision: 0.85\n",
      "Epoch 15/50, Train Loss: 0.14, Test Loss: 0.52, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 16/50, Train Loss: 0.13, Test Loss: 0.53, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 17/50, Train Loss: 0.12, Test Loss: 0.56, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 18/50, Train Loss: 0.11, Test Loss: 0.58, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 19/50, Train Loss: 0.10, Test Loss: 0.64, Test AUC: 0.91, Test Precision: 0.84\n",
      "Epoch 20/50, Train Loss: 0.10, Test Loss: 0.61, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 21/50, Train Loss: 0.10, Test Loss: 0.61, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 22/50, Train Loss: 0.09, Test Loss: 0.66, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 23/50, Train Loss: 0.08, Test Loss: 0.68, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 24/50, Train Loss: 0.08, Test Loss: 0.66, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 25/50, Train Loss: 0.08, Test Loss: 0.67, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 26/50, Train Loss: 0.07, Test Loss: 0.67, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 27/50, Train Loss: 0.07, Test Loss: 0.68, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 28/50, Train Loss: 0.07, Test Loss: 0.65, Test AUC: 0.92, Test Precision: 0.86\n",
      "Epoch 29/50, Train Loss: 0.06, Test Loss: 0.70, Test AUC: 0.92, Test Precision: 0.87\n",
      "Epoch 30/50, Train Loss: 0.07, Test Loss: 0.68, Test AUC: 0.92, Test Precision: 0.88\n",
      "Epoch 31/50, Train Loss: 0.06, Test Loss: 0.73, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 32/50, Train Loss: 0.05, Test Loss: 0.76, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 33/50, Train Loss: 0.06, Test Loss: 0.76, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 34/50, Train Loss: 0.05, Test Loss: 0.78, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 35/50, Train Loss: 0.06, Test Loss: 0.77, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 36/50, Train Loss: 0.05, Test Loss: 0.80, Test AUC: 0.91, Test Precision: 0.85\n",
      "Epoch 37/50, Train Loss: 0.05, Test Loss: 0.78, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 38/50, Train Loss: 0.04, Test Loss: 0.80, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 39/50, Train Loss: 0.04, Test Loss: 0.82, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 40/50, Train Loss: 0.05, Test Loss: 0.81, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 41/50, Train Loss: 0.04, Test Loss: 0.81, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 42/50, Train Loss: 0.04, Test Loss: 0.81, Test AUC: 0.91, Test Precision: 0.86\n",
      "Epoch 43/50, Train Loss: 0.04, Test Loss: 0.82, Test AUC: 0.92, Test Precision: 0.87\n",
      "Epoch 44/50, Train Loss: 0.04, Test Loss: 0.83, Test AUC: 0.92, Test Precision: 0.86\n",
      "Epoch 45/50, Train Loss: 0.04, Test Loss: 0.82, Test AUC: 0.91, Test Precision: 0.87\n",
      "Epoch 46/50, Train Loss: 0.04, Test Loss: 0.82, Test AUC: 0.92, Test Precision: 0.86\n",
      "Epoch 47/50, Train Loss: 0.03, Test Loss: 0.84, Test AUC: 0.92, Test Precision: 0.86\n",
      "Epoch 48/50, Train Loss: 0.03, Test Loss: 0.82, Test AUC: 0.92, Test Precision: 0.87\n",
      "Epoch 49/50, Train Loss: 0.04, Test Loss: 0.83, Test AUC: 0.92, Test Precision: 0.87\n",
      "Epoch 50/50, Train Loss: 0.03, Test Loss: 0.84, Test AUC: 0.92, Test Precision: 0.86\n",
      "Best Test AUC was 0.92 at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_28516\\1777769994.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"best_model.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Test Accuracy: 0.86\n",
      "Confusion Matrix:\n",
      "[[349  95]\n",
      " [ 71 589]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.83      0.79      0.81       444\n",
      "     Class 1       0.86      0.89      0.88       660\n",
      "\n",
      "    accuracy                           0.85      1104\n",
      "   macro avg       0.85      0.84      0.84      1104\n",
      "weighted avg       0.85      0.85      0.85      1104\n",
      "\n",
      "ROC AUC: 0.92\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc, precision_score\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "seed = 3407\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load downstream task dataset\n",
    "file_path = 'data3.csv'\n",
    "data2_df = pd.read_csv(file_path)\n",
    "\n",
    "# Split training and test sets with 8:2 ratio\n",
    "train_df, test_df = train_test_split(data2_df, test_size=0.1, random_state=seed)\n",
    "\n",
    "# Function to generate enhanced representations using upstream model\n",
    "def generate_enhanced_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-1][:, 0, :]\n",
    "    predictions = torch.sigmoid(outputs.logits)\n",
    "    enhanced_embeddings = torch.cat((embeddings, predictions), dim=1)\n",
    "    return enhanced_embeddings\n",
    "\n",
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, model):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        embeddings = generate_enhanced_embeddings([text], self.tokenizer, self.model)\n",
    "        return embeddings.squeeze(), label\n",
    "\n",
    "# Define simple binary classifier\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Load pre-trained model and tokenizer (load only once)\n",
    "model_path = \"saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.output_hidden_states = True\n",
    "\n",
    "# Initialize model and classifier\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config).to(device)\n",
    "classifier = SimpleClassifier(config.hidden_size + config.num_labels).to(device)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = TextDataset(train_df['compound'].tolist(), train_df['label'].tolist(), tokenizer, model)\n",
    "test_dataset = TextDataset(test_df['compound'].tolist(), test_df['label'].tolist(), tokenizer, model)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Put parameters of both model and classifier in the same optimizer, using AdamW\n",
    "optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=0.000260006006712908)\n",
    "\n",
    "# Use CosineAnnealingLR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=64)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Track the model with best AUC on test set\n",
    "best_test_auc = 0.0\n",
    "best_epoch = 0  # Record the best epoch\n",
    "\n",
    "# Train model and classifier\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for embeddings, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device, dtype=torch.float32)\n",
    "            outputs = classifier(embeddings).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # Calculate loss, AUC and precision on test set\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    test_outputs = []\n",
    "    test_labels = []\n",
    "    test_predictions = []\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in test_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device, dtype=torch.float32)\n",
    "            outputs = classifier(embeddings).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Convert prediction probabilities to binary labels\n",
    "            predictions = torch.sigmoid(outputs) > 0.5\n",
    "            test_outputs.extend(outputs.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    # Calculate AUC\n",
    "    fpr, tpr, _ = roc_curve(test_labels, test_outputs)\n",
    "    test_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Calculate precision\n",
    "    test_precision = precision_score(test_labels, test_predictions)\n",
    "\n",
    "    # Save the best performing model on test set, and immediately save to disk\n",
    "    if test_auc > best_test_auc:\n",
    "        best_test_auc = test_auc\n",
    "        best_epoch = epoch + 1  # Record the epoch of the best model\n",
    "\n",
    "        # Immediately save the best model to file\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'classifier_state_dict': classifier.state_dict(),\n",
    "            'test_auc': best_test_auc,\n",
    "            'epoch': best_epoch\n",
    "        }, \"best_model.pth\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training and test results for each epoch, including precision\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}, Test AUC: {test_auc:.2f}, Test Precision: {test_precision:.2f}\")\n",
    "\n",
    "# Print the epoch with the best model\n",
    "print(f\"Best Test AUC was {best_test_auc:.2f} at epoch {best_epoch}\")\n",
    "\n",
    "# Load the best performing model on test set\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "\n",
    "# Perform final evaluation after loading the best model\n",
    "model.eval()\n",
    "classifier.eval()\n",
    "test_accuracy = 0\n",
    "all_test_labels = []\n",
    "all_test_predictions = []\n",
    "all_test_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in test_loader:\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device, dtype=torch.float32)\n",
    "        outputs = classifier(embeddings).squeeze()\n",
    "        test_predictions = torch.sigmoid(outputs) > 0.5\n",
    "        test_accuracy += accuracy_score(labels.cpu(), test_predictions.cpu())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "        all_test_predictions.extend(test_predictions.cpu().numpy())\n",
    "        all_test_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "test_accuracy /= len(test_loader)\n",
    "\n",
    "# Calculate confusion matrix and other evaluation metrics\n",
    "conf_matrix = confusion_matrix(all_test_labels, all_test_predictions)\n",
    "class_report = classification_report(all_test_labels, all_test_predictions, target_names=['Class 0', 'Class 1'])\n",
    "accuracy = accuracy_score(all_test_labels, all_test_predictions)\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(all_test_labels, all_test_outputs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"\\nBest Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"\\nClassification Report:\\n{class_report}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a15cb-e58e-4819-af2c-76bb7d9c2c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syq",
   "language": "python",
   "name": "syq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
