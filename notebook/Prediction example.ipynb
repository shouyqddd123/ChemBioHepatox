{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e69d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the probability of hepatotoxicity of a chemical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1277141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\93757\\.conda\\envs\\storch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Predict the probability of hepatotoxicity of a chemical\n",
    "import torch  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig  \n",
    "import torch.nn as nn  \n",
    "# Load pre-trained model and tokenizer\n",
    "model_path = \"saved_model\"  # Make sure the path points to the correct model folder\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  \n",
    "config = AutoConfig.from_pretrained(model_path)  \n",
    "# Ensure the model outputs hidden states\n",
    "config.output_hidden_states = True  \n",
    "# Load model - directly use CPU\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)  \n",
    "# Define classifier\n",
    "class SimpleClassifier(nn.Module):  \n",
    "    def __init__(self, input_dim):  \n",
    "        super(SimpleClassifier, self).__init__()  \n",
    "        self.linear = nn.Linear(input_dim, 1)  \n",
    "    \n",
    "    def forward(self, x):  \n",
    "        return self.linear(x)  \n",
    "# Dynamically set the input dimension of the classifier\n",
    "classifier = SimpleClassifier(config.hidden_size + config.num_labels)  \n",
    "# Load model weights - add map_location parameter to ensure loading to CPU\n",
    "checkpoint = torch.load(\"best_model.pth\", map_location=torch.device('cpu'))  \n",
    "model.load_state_dict(checkpoint['model_state_dict'])  \n",
    "classifier.load_state_dict(checkpoint['classifier_state_dict'])  \n",
    "# Set to evaluation mode\n",
    "model.eval()  \n",
    "classifier.eval()  \n",
    "# Define function to generate enhanced embeddings\n",
    "def generate_enhanced_embeddings(smiles, tokenizer, model):  \n",
    "    # Encode SMILES input\n",
    "    inputs = tokenizer(smiles, padding=True, truncation=True, return_tensors=\"pt\")  \n",
    "    outputs = model(**inputs)  \n",
    "    \n",
    "    # Extract embeddings from the last layer\n",
    "    embeddings = outputs.hidden_states[-1][:, 0, :]  \n",
    "    \n",
    "    # Get model prediction probabilities\n",
    "    predictions = torch.sigmoid(outputs.logits)  \n",
    "    \n",
    "    # Concatenate embeddings and prediction probabilities\n",
    "    enhanced_embeddings = torch.cat((embeddings, predictions), dim=-1)  \n",
    "    \n",
    "    return enhanced_embeddings, predictions  \n",
    "# Define a prediction function\n",
    "def predict(smiles):  \n",
    "    # Get enhanced embeddings and prediction probabilities\n",
    "    enhanced_embeddings, predictions = generate_enhanced_embeddings(smiles, tokenizer, model)  \n",
    "    \n",
    "    # Input enhanced embeddings to the classifier for final prediction\n",
    "    classifier_predictions = classifier(enhanced_embeddings)  \n",
    "    \n",
    "    # Get the final prediction probability from the classifier\n",
    "    final_predictions = torch.sigmoid(classifier_predictions).item()  \n",
    "    \n",
    "    # Output prediction probability (rounded to two decimal places)\n",
    "    return round(final_predictions, 2)  \n",
    "# Input a SMILES expression\n",
    "smiles = \"COC1=C2C3=C(C(=O)CC3)C(=O)OC2=C4[C@@H]5C=CO[C@@H]5OC4=C1\"  # Example SMILES structure\n",
    "prediction = predict(smiles)  \n",
    "# Output prediction result\n",
    "print(f\"Predicted probability: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203a248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a3acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction probability values of 19 assays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f68e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caspase-3/7 HepG2 qHTS: 0.04\n",
      "CYP1A2 Antag qHTS: 0.04\n",
      "CYP2C19 Antag qHTS: 0.05\n",
      "CYP2C9 Antag qHTS: 1.00\n",
      "CYP3A4 Antag Reporter qHTS: 0.99\n",
      "CYP3A7 Antag Cell qHTS: 0.99\n",
      "ARE Agon qHTS: 1.00\n",
      "MMP qHTS: 0.97\n",
      "ER Stress: 0.04\n",
      "ER-beta Agon qHTS: Summary: 0.98\n",
      "PPARg Agon qHTS: Summary: 0.99\n",
      "RAR Agon qHTS: 0.98\n",
      "ERR Antag qHTS: 0.06\n",
      "GR Antag qHTS: 0.99\n",
      "PPARd Antag qHTS: 0.03\n",
      "PPARg Antag Summary qHTS: 1.00\n",
      "TR Antag Summary qHTS: 0.05\n",
      "MDR-1: 0.05\n",
      "HPGD Inhib qHTS: 0.98\n"
     ]
    }
   ],
   "source": [
    "#Prediction probability values of 19 assays\n",
    "import torch  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification  \n",
    "# Define label names  \n",
    "labels = [  \n",
    "    \"Caspase-3/7 HepG2 qHTS\", \"CYP1A2 Antag qHTS\", \"CYP2C19 Antag qHTS\",   \n",
    "    \"CYP2C9 Antag qHTS\", \"CYP3A4 Antag Reporter qHTS\", \"CYP3A7 Antag Cell qHTS\",   \n",
    "    \"ARE Agon qHTS\", \"MMP qHTS\", \"ER Stress\", \"ER-beta Agon qHTS: Summary\",   \n",
    "    \"PPARg Agon qHTS: Summary\", \"RAR Agon qHTS\", \"ERR Antag qHTS\", \"GR Antag qHTS\",   \n",
    "    \"PPARd Antag qHTS\", \"PPARg Antag Summary qHTS\", \"TR Antag Summary qHTS\", \"MDR-1\",   \n",
    "    \"HPGD Inhib qHTS\"  \n",
    "]  \n",
    "# Load model and tokenizer  \n",
    "model_path = \"saved_model\"  # Ensure the path points to the correct model folder  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)  \n",
    "# Load model weights - add map_location parameter to ensure loading to CPU  \n",
    "model_save_path = \"best_model.pth\"  \n",
    "checkpoint = torch.load(model_save_path, map_location=torch.device('cpu'))  \n",
    "model.load_state_dict(checkpoint['model_state_dict'])  \n",
    "model.eval()  \n",
    "# Define function to generate prediction probabilities  \n",
    "def get_predictions(smiles):  \n",
    "    inputs = tokenizer(smiles, padding=True, truncation=True, return_tensors=\"pt\")  \n",
    "    with torch.no_grad():  \n",
    "        outputs = model(**inputs)  \n",
    "        # Use sigmoid to calculate probabilities  \n",
    "        predictions = torch.sigmoid(outputs.logits).numpy().flatten()  \n",
    "    # Round prediction probabilities to two decimal places and create a dictionary output  \n",
    "    predicted_probabilities = {labels[i]: predictions[i] for i in range(len(labels))}  \n",
    "    return predicted_probabilities  \n",
    "# Input SMILES expression  \n",
    "smiles_input = \"COC1=C2C3=C(C(=O)CC3)C(=O)OC2=C4[C@@H]5C=CO[C@@H]5OC4=C1\"  # Example SMILES, can be replaced with any SMILES  \n",
    "predictions = get_predictions(smiles_input)  \n",
    "# Output prediction results  \n",
    "for label, prob in predictions.items():  \n",
    "    print(f\"{label}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b9d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storch",
   "language": "python",
   "name": "storch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
