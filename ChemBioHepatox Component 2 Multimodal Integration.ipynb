{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc06b40-9ec1-468f-8a92-05f4f6698610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-task Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2930805-88a2-48ce-be76-f7c0c547e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\syq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and model loaded successfully.\n",
      "Epoch 1/100, Training Loss: 0.0452306866645813\n",
      "Epoch 1/100, Validation Loss: 0.04944154433906078, Accuracy: 0.5821389195148843, AUC: 0.6111940590109183\n",
      "Epoch 2/100, Training Loss: 0.046448223758488894\n",
      "Epoch 2/100, Validation Loss: 0.04912199266254902, Accuracy: 0.607497243660419, AUC: 0.6551910646602873\n",
      "Epoch 3/100, Training Loss: 0.045555304270237684\n",
      "Epoch 3/100, Validation Loss: 0.04879484139382839, Accuracy: 0.6218302094818081, AUC: 0.6917124765295148\n",
      "Epoch 4/100, Training Loss: 0.04427354782819748\n",
      "Epoch 4/100, Validation Loss: 0.04845667444169521, Accuracy: 0.6306504961411246, AUC: 0.7229896616753968\n",
      "Epoch 5/100, Training Loss: 0.045422644820064306\n",
      "Epoch 5/100, Validation Loss: 0.048111945390701294, Accuracy: 0.643329658213892, AUC: 0.7464128460932568\n",
      "Epoch 6/100, Training Loss: 0.044879944529384375\n",
      "Epoch 6/100, Validation Loss: 0.047745537012815475, Accuracy: 0.6615214994487321, AUC: 0.7629259905932968\n",
      "Epoch 7/100, Training Loss: 0.04411932546645403\n",
      "Epoch 7/100, Validation Loss: 0.0473642498254776, Accuracy: 0.6692392502756339, AUC: 0.7744386106706646\n",
      "Epoch 8/100, Training Loss: 0.04397380305454135\n",
      "Epoch 8/100, Validation Loss: 0.04695322923362255, Accuracy: 0.6758544652701213, AUC: 0.7820951816867491\n",
      "Epoch 9/100, Training Loss: 0.04381362674757838\n",
      "Epoch 9/100, Validation Loss: 0.0465181078761816, Accuracy: 0.6824696802646086, AUC: 0.7878237281721915\n",
      "Epoch 10/100, Training Loss: 0.04208422498777509\n",
      "Epoch 10/100, Validation Loss: 0.04605667293071747, Accuracy: 0.6841234840132304, AUC: 0.7915512422647655\n",
      "Epoch 11/100, Training Loss: 0.042015853337943554\n",
      "Epoch 11/100, Validation Loss: 0.045569343492388725, Accuracy: 0.6901874310915105, AUC: 0.7942309392924201\n",
      "Epoch 12/100, Training Loss: 0.04308092035353184\n",
      "Epoch 12/100, Validation Loss: 0.04505900479853153, Accuracy: 0.6907386990077178, AUC: 0.7960314571341021\n",
      "Epoch 13/100, Training Loss: 0.0419037570245564\n",
      "Epoch 13/100, Validation Loss: 0.044516799971461296, Accuracy: 0.6934950385887542, AUC: 0.7972283747627245\n",
      "Epoch 14/100, Training Loss: 0.04129190556704998\n",
      "Epoch 14/100, Validation Loss: 0.04397832602262497, Accuracy: 0.6962513781697905, AUC: 0.7980725924372081\n",
      "Epoch 15/100, Training Loss: 0.041259550023823977\n",
      "Epoch 15/100, Validation Loss: 0.04343314655125141, Accuracy: 0.6957001102535832, AUC: 0.7981713072931771\n",
      "Epoch 16/100, Training Loss: 0.04017655365169048\n",
      "Epoch 16/100, Validation Loss: 0.042903829365968704, Accuracy: 0.6962513781697905, AUC: 0.7986916168465128\n",
      "Epoch 17/100, Training Loss: 0.039673279505223036\n",
      "Epoch 17/100, Validation Loss: 0.042396239936351776, Accuracy: 0.6979051819184123, AUC: 0.7991749083288604\n",
      "Epoch 18/100, Training Loss: 0.039662881288677454\n",
      "Epoch 18/100, Validation Loss: 0.04193769954144955, Accuracy: 0.6990077177508269, AUC: 0.7992304354353428\n",
      "Epoch 19/100, Training Loss: 0.03886879328638315\n",
      "Epoch 19/100, Validation Loss: 0.04153645411133766, Accuracy: 0.7006615214994487, AUC: 0.799594446466728\n",
      "Epoch 20/100, Training Loss: 0.038944331696256995\n",
      "Epoch 20/100, Validation Loss: 0.04119143262505531, Accuracy: 0.701212789415656, AUC: 0.79974663186968\n",
      "Epoch 21/100, Training Loss: 0.037287751911208034\n",
      "Epoch 21/100, Validation Loss: 0.040909988805651665, Accuracy: 0.6984564498346196, AUC: 0.8000201542830937\n",
      "Epoch 22/100, Training Loss: 0.03897718479856849\n",
      "Epoch 22/100, Validation Loss: 0.04067695327103138, Accuracy: 0.701212789415656, AUC: 0.8006247827759028\n",
      "Epoch 23/100, Training Loss: 0.0382794588804245\n",
      "Epoch 23/100, Validation Loss: 0.04048899747431278, Accuracy: 0.7017640573318633, AUC: 0.8014145016236537\n",
      "Epoch 24/100, Training Loss: 0.03825762588530779\n",
      "Epoch 24/100, Validation Loss: 0.040352603420615196, Accuracy: 0.7023153252480706, AUC: 0.8019368677364889\n",
      "Epoch 25/100, Training Loss: 0.03591261222027242\n",
      "Epoch 25/100, Validation Loss: 0.04025078006088734, Accuracy: 0.7039691289966924, AUC: 0.8026093626927768\n",
      "Epoch 26/100, Training Loss: 0.03831158671528101\n",
      "Epoch 26/100, Validation Loss: 0.040153492242097855, Accuracy: 0.705071664829107, AUC: 0.8036540949184471\n",
      "Epoch 27/100, Training Loss: 0.03798624221235514\n",
      "Epoch 27/100, Validation Loss: 0.04006451927125454, Accuracy: 0.7072767364939361, AUC: 0.804285458684748\n",
      "Epoch 28/100, Training Loss: 0.037085161777213216\n",
      "Epoch 28/100, Validation Loss: 0.039982061833143234, Accuracy: 0.7089305402425579, AUC: 0.8051738923884677\n",
      "Epoch 29/100, Training Loss: 0.0378019236959517\n",
      "Epoch 29/100, Validation Loss: 0.0398960392922163, Accuracy: 0.7094818081587652, AUC: 0.8068541014994374\n",
      "Epoch 30/100, Training Loss: 0.03740585362538695\n",
      "Epoch 30/100, Validation Loss: 0.039816634729504585, Accuracy: 0.7094818081587652, AUC: 0.8079523042720911\n",
      "Epoch 31/100, Training Loss: 0.036405477207154036\n",
      "Epoch 31/100, Validation Loss: 0.0397400576621294, Accuracy: 0.7111356119073869, AUC: 0.8088078330238212\n",
      "Epoch 32/100, Training Loss: 0.03710528207011521\n",
      "Epoch 32/100, Validation Loss: 0.039682112634181976, Accuracy: 0.7122381477398015, AUC: 0.8092767285896731\n",
      "Epoch 33/100, Training Loss: 0.036261860746890306\n",
      "Epoch 33/100, Validation Loss: 0.039624596014618874, Accuracy: 0.7127894156560088, AUC: 0.8099841850574501\n",
      "Epoch 34/100, Training Loss: 0.037239110097289085\n",
      "Epoch 34/100, Validation Loss: 0.03955704905092716, Accuracy: 0.7127894156560088, AUC: 0.8106628496922359\n",
      "Epoch 35/100, Training Loss: 0.03628209140151739\n",
      "Epoch 35/100, Validation Loss: 0.039489464834332466, Accuracy: 0.7122381477398015, AUC: 0.8114176070284976\n",
      "Epoch 36/100, Training Loss: 0.036210281774401665\n",
      "Epoch 36/100, Validation Loss: 0.039421211928129196, Accuracy: 0.7133406835722161, AUC: 0.8124541130161708\n",
      "Epoch 37/100, Training Loss: 0.03548040403984487\n",
      "Epoch 37/100, Validation Loss: 0.03933310508728027, Accuracy: 0.7144432194046306, AUC: 0.8137805938932522\n",
      "Epoch 38/100, Training Loss: 0.03542911121621728\n",
      "Epoch 38/100, Validation Loss: 0.039265330880880356, Accuracy: 0.717199558985667, AUC: 0.8145661996220044\n",
      "Epoch 39/100, Training Loss: 0.03578566643409431\n",
      "Epoch 39/100, Validation Loss: 0.039148371666669846, Accuracy: 0.7183020948180816, AUC: 0.8154854817182142\n",
      "Epoch 40/100, Training Loss: 0.036416296381503344\n",
      "Epoch 40/100, Validation Loss: 0.0390543919056654, Accuracy: 0.7199558985667034, AUC: 0.8165754582528704\n",
      "Epoch 41/100, Training Loss: 0.035062808310613036\n",
      "Epoch 41/100, Validation Loss: 0.038954541087150574, Accuracy: 0.721058434399118, AUC: 0.8173631205411219\n",
      "Epoch 42/100, Training Loss: 0.034831087570637465\n",
      "Epoch 42/100, Validation Loss: 0.03885161131620407, Accuracy: 0.7221609702315325, AUC: 0.8182896005955798\n",
      "Epoch 43/100, Training Loss: 0.03380029578693211\n",
      "Epoch 43/100, Validation Loss: 0.03874018415808678, Accuracy: 0.7216097023153253, AUC: 0.8192099109715394\n",
      "Epoch 44/100, Training Loss: 0.03519111964851618\n",
      "Epoch 44/100, Validation Loss: 0.03865025006234646, Accuracy: 0.7232635060639471, AUC: 0.8199050280823201\n",
      "Epoch 45/100, Training Loss: 0.03532376233488321\n",
      "Epoch 45/100, Validation Loss: 0.03851706720888615, Accuracy: 0.7265711135611908, AUC: 0.8214392214688357\n",
      "Epoch 46/100, Training Loss: 0.03540770290419459\n",
      "Epoch 46/100, Validation Loss: 0.03840890619903803, Accuracy: 0.7260198456449835, AUC: 0.8225435939199874\n",
      "Epoch 47/100, Training Loss: 0.03507222421467304\n",
      "Epoch 47/100, Validation Loss: 0.03830193169414997, Accuracy: 0.7282249173098125, AUC: 0.8237425681081092\n",
      "Epoch 48/100, Training Loss: 0.035428466042503715\n",
      "Epoch 48/100, Validation Loss: 0.0381831768900156, Accuracy: 0.7265711135611908, AUC: 0.8249477119747288\n",
      "Epoch 49/100, Training Loss: 0.036011900287121534\n",
      "Epoch 49/100, Validation Loss: 0.03805707208812237, Accuracy: 0.7304299889746417, AUC: 0.8262207223048273\n",
      "Epoch 50/100, Training Loss: 0.03446375695057213\n",
      "Epoch 50/100, Validation Loss: 0.03792629670351744, Accuracy: 0.7342888643880926, AUC: 0.8273518300294704\n",
      "Epoch 51/100, Training Loss: 0.03326672362163663\n",
      "Epoch 51/100, Validation Loss: 0.03780113905668259, Accuracy: 0.7392502756339581, AUC: 0.8282402637331902\n",
      "Epoch 52/100, Training Loss: 0.034131243359297514\n",
      "Epoch 52/100, Validation Loss: 0.03767407312989235, Accuracy: 0.7403528114663727, AUC: 0.8294227854453171\n",
      "Epoch 53/100, Training Loss: 0.03404347947798669\n",
      "Epoch 53/100, Validation Loss: 0.037544477730989456, Accuracy: 0.7425578831312017, AUC: 0.8303770290530159\n",
      "Epoch 54/100, Training Loss: 0.03409509244374931\n",
      "Epoch 54/100, Validation Loss: 0.037404597736895084, Accuracy: 0.7442116868798236, AUC: 0.8316808877756047\n",
      "Epoch 55/100, Training Loss: 0.03336447291076183\n",
      "Epoch 55/100, Validation Loss: 0.037256249226629734, Accuracy: 0.7464167585446527, AUC: 0.8332860324648481\n",
      "Epoch 56/100, Training Loss: 0.033291093772277236\n",
      "Epoch 56/100, Validation Loss: 0.03714018780738115, Accuracy: 0.7508269018743109, AUC: 0.8341158542228364\n",
      "Epoch 57/100, Training Loss: 0.03420399874448776\n",
      "Epoch 57/100, Validation Loss: 0.037009178660809994, Accuracy: 0.7519294377067255, AUC: 0.8349652132960684\n",
      "Epoch 58/100, Training Loss: 0.032193101942539215\n",
      "Epoch 58/100, Validation Loss: 0.03686520457267761, Accuracy: 0.7552370452039692, AUC: 0.8358680429162837\n",
      "Epoch 59/100, Training Loss: 0.03306091856211424\n",
      "Epoch 59/100, Validation Loss: 0.03674243204295635, Accuracy: 0.7541345093715546, AUC: 0.8368099471669863\n",
      "Epoch 60/100, Training Loss: 0.03307443531230092\n",
      "Epoch 60/100, Validation Loss: 0.03661520313471556, Accuracy: 0.756890848952591, AUC: 0.8378834712256478\n",
      "Epoch 61/100, Training Loss: 0.03325167787261307\n",
      "Epoch 61/100, Validation Loss: 0.0364433815702796, Accuracy: 0.7585446527012127, AUC: 0.8395780762531132\n",
      "Epoch 62/100, Training Loss: 0.03334385226480663\n",
      "Epoch 62/100, Validation Loss: 0.03630912024527788, Accuracy: 0.7579933847850056, AUC: 0.8406803921447654\n",
      "Epoch 63/100, Training Loss: 0.03205565409734845\n",
      "Epoch 63/100, Validation Loss: 0.03621574770659208, Accuracy: 0.7646085997794928, AUC: 0.8414166404455331\n",
      "Epoch 64/100, Training Loss: 0.03150713490322232\n",
      "Epoch 64/100, Validation Loss: 0.03613411448895931, Accuracy: 0.7690187431091511, AUC: 0.8419965902243501\n",
      "Epoch 65/100, Training Loss: 0.03188386047258973\n",
      "Epoch 65/100, Validation Loss: 0.03599881753325462, Accuracy: 0.7701212789415656, AUC: 0.8433559760534212\n",
      "Epoch 66/100, Training Loss: 0.03195064072497189\n",
      "Epoch 66/100, Validation Loss: 0.03581801522523165, Accuracy: 0.7695700110253583, AUC: 0.8452500673523236\n",
      "Epoch 67/100, Training Loss: 0.03165795328095555\n",
      "Epoch 67/100, Validation Loss: 0.03569222427904606, Accuracy: 0.7701212789415656, AUC: 0.846358552922474\n",
      "Epoch 68/100, Training Loss: 0.031040085712447762\n",
      "Epoch 68/100, Validation Loss: 0.03561070468276739, Accuracy: 0.7761852260198456, AUC: 0.8470516134737551\n",
      "Epoch 69/100, Training Loss: 0.030177250504493713\n",
      "Epoch 69/100, Validation Loss: 0.03555324114859104, Accuracy: 0.7772877618522602, AUC: 0.8474588122546267\n",
      "Epoch 70/100, Training Loss: 0.03123900294303894\n",
      "Epoch 70/100, Validation Loss: 0.03545844741165638, Accuracy: 0.778941565600882, AUC: 0.8485138272777939\n",
      "Epoch 71/100, Training Loss: 0.031030755722895265\n",
      "Epoch 71/100, Validation Loss: 0.035360317677259445, Accuracy: 0.7800441014332966, AUC: 0.8495338807894722\n",
      "Epoch 72/100, Training Loss: 0.02992769656702876\n",
      "Epoch 72/100, Validation Loss: 0.03531190659850836, Accuracy: 0.7816979051819184, AUC: 0.8498732131068649\n",
      "Epoch 73/100, Training Loss: 0.03029012936167419\n",
      "Epoch 73/100, Validation Loss: 0.035272449254989624, Accuracy: 0.7850055126791621, AUC: 0.8503585611487119\n",
      "Epoch 74/100, Training Loss: 0.0301458400208503\n",
      "Epoch 74/100, Validation Loss: 0.035158208571374416, Accuracy: 0.7844542447629548, AUC: 0.8514793860758583\n",
      "Epoch 75/100, Training Loss: 0.030132729560136795\n",
      "Epoch 75/100, Validation Loss: 0.03503970429301262, Accuracy: 0.7794928335170893, AUC: 0.852631059395495\n",
      "Epoch 76/100, Training Loss: 0.029009349178522825\n",
      "Epoch 76/100, Validation Loss: 0.03499291930347681, Accuracy: 0.7794928335170893, AUC: 0.8531781042223223\n",
      "Epoch 77/100, Training Loss: 0.030156095512211323\n",
      "Epoch 77/100, Validation Loss: 0.035002768971025944, Accuracy: 0.7872105843439912, AUC: 0.8530567672118606\n",
      "Epoch 78/100, Training Loss: 0.030238240724429488\n",
      "Epoch 78/100, Validation Loss: 0.03499087691307068, Accuracy: 0.7877618522601985, AUC: 0.8533138371492794\n",
      "Epoch 79/100, Training Loss: 0.03018219512887299\n",
      "Epoch 79/100, Validation Loss: 0.03484707698225975, Accuracy: 0.7877618522601985, AUC: 0.8547184672873365\n",
      "Epoch 80/100, Training Loss: 0.030380386859178543\n",
      "Epoch 80/100, Validation Loss: 0.03477693721652031, Accuracy: 0.7883131201764058, AUC: 0.855337491696641\n",
      "Epoch 81/100, Training Loss: 0.03033365565352142\n",
      "Epoch 81/100, Validation Loss: 0.034677243791520596, Accuracy: 0.7861080485115767, AUC: 0.8562547172333517\n",
      "Epoch 82/100, Training Loss: 0.028568527195602655\n",
      "Epoch 82/100, Validation Loss: 0.03461890481412411, Accuracy: 0.7850055126791621, AUC: 0.8570793975925914\n",
      "Epoch 83/100, Training Loss: 0.029352702666074038\n",
      "Epoch 83/100, Validation Loss: 0.03459915705025196, Accuracy: 0.7866593164277839, AUC: 0.8575997071459273\n",
      "Epoch 84/100, Training Loss: 0.028523148503154516\n",
      "Epoch 84/100, Validation Loss: 0.03455581609159708, Accuracy: 0.7872105843439912, AUC: 0.85822490123373\n",
      "Epoch 85/100, Training Loss: 0.028561713872477412\n",
      "Epoch 85/100, Validation Loss: 0.034502373076975346, Accuracy: 0.7861080485115767, AUC: 0.8588644912380281\n",
      "Epoch 86/100, Training Loss: 0.027665633475407958\n",
      "Epoch 86/100, Validation Loss: 0.03448803909122944, Accuracy: 0.7883131201764058, AUC: 0.8591626923654343\n",
      "Epoch 87/100, Training Loss: 0.027328884694725275\n",
      "Epoch 87/100, Validation Loss: 0.03446343820542097, Accuracy: 0.7894156560088202, AUC: 0.8594855722068323\n",
      "Epoch 88/100, Training Loss: 0.027903711888939142\n",
      "Epoch 88/100, Validation Loss: 0.034474194049835205, Accuracy: 0.7883131201764058, AUC: 0.8595822305033018\n",
      "Epoch 89/100, Training Loss: 0.027476562885567546\n",
      "Epoch 89/100, Validation Loss: 0.034484127536416054, Accuracy: 0.7883131201764058, AUC: 0.8596542100857789\n",
      "Epoch 90/100, Training Loss: 0.02766003180295229\n",
      "Epoch 90/100, Validation Loss: 0.03451633267104626, Accuracy: 0.7899669239250275, AUC: 0.8597385290252525\n",
      "Epoch 91/100, Training Loss: 0.026666571502573788\n",
      "Epoch 91/100, Validation Loss: 0.03449850436300039, Accuracy: 0.7905181918412348, AUC: 0.8599729768081784\n",
      "Epoch 92/100, Training Loss: 0.027714537689462304\n",
      "Epoch 92/100, Validation Loss: 0.03445956949144602, Accuracy: 0.7910694597574421, AUC: 0.8604459854930293\n",
      "Epoch 93/100, Training Loss: 0.027921219123527408\n",
      "Epoch 93/100, Validation Loss: 0.034519617445766926, Accuracy: 0.7894156560088202, AUC: 0.8602917435305779\n",
      "Epoch 94/100, Training Loss: 0.02787219430319965\n",
      "Epoch 94/100, Validation Loss: 0.034557491540908813, Accuracy: 0.7921719955898566, AUC: 0.8601704065201161\n",
      "Epoch 95/100, Training Loss: 0.02717420202679932\n",
      "Epoch 95/100, Validation Loss: 0.03450906276702881, Accuracy: 0.7894156560088202, AUC: 0.8605878880984845\n",
      "Epoch 96/100, Training Loss: 0.027586005860939622\n",
      "Epoch 96/100, Validation Loss: 0.03463332820683718, Accuracy: 0.7938257993384785, AUC: 0.8601313318896285\n",
      "Epoch 97/100, Training Loss: 0.02782398066483438\n",
      "Epoch 97/100, Validation Loss: 0.03455197624862194, Accuracy: 0.7916207276736494, AUC: 0.8607421300609359\n",
      "Epoch 98/100, Training Loss: 0.027304172050207853\n",
      "Epoch 98/100, Validation Loss: 0.03462720289826393, Accuracy: 0.7943770672546858, AUC: 0.8605220781945052\n",
      "Epoch 99/100, Training Loss: 0.02673761104233563\n",
      "Epoch 99/100, Validation Loss: 0.034651451744139194, Accuracy: 0.7932745314222712, AUC: 0.8605796618604871\n",
      "Epoch 100/100, Training Loss: 0.02669577905908227\n",
      "Epoch 100/100, Validation Loss: 0.0346009936183691, Accuracy: 0.7927232635060639, AUC: 0.8609909737603574\n",
      "Enhanced embeddings and predictions obtained successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load multi-task dataset\n",
    "file_path = 'cti.csv'\n",
    "multi_task_df = pd.read_csv(file_path)\n",
    "\n",
    "# Check and process null values\n",
    "multi_task_df.iloc[:, 1:] = multi_task_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')  # Convert non-numeric data to NaN\n",
    "labels = multi_task_df.iloc[:, 1:].values\n",
    "\n",
    "# Create label masks, marking positions of non-null values\n",
    "label_masks = ~pd.isna(labels)\n",
    "labels = np.where(pd.isna(labels), -1, labels)  # Fill NaN values with -1\n",
    "labels = labels.astype(int)  # Convert to integer type for classification tasks\n",
    "labels = torch.tensor(labels, dtype=torch.long)  # Use long type as classification tasks typically use integer labels\n",
    "label_masks = torch.tensor(label_masks, dtype=torch.float)\n",
    "\n",
    "# Specify local model path\n",
    "model_path = \"DeepChem/ChemBERTa-77M-MTR\"\n",
    "\n",
    "# Load tokenizer and model configuration from local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.num_labels = labels.shape[1]  # Dynamically determine the number of labels\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True  # Ensure attention weights are output\n",
    "\n",
    "# Load model with updated configuration from local path\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "\n",
    "print(\"Tokenizer and model loaded successfully.\")\n",
    "\n",
    "# Tokenize input data\n",
    "inputs = tokenizer(list(multi_task_df.iloc[:, 0]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "labels = labels.to(device)\n",
    "label_masks = label_masks.to(device)\n",
    "\n",
    "# Split inputs into two parts\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Split into training and test sets\n",
    "train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels, train_label_masks, test_label_masks = train_test_split(\n",
    "    input_ids, attention_mask, labels, label_masks, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Recombine into training and test set inputs\n",
    "train_inputs = {'input_ids': train_input_ids, 'attention_mask': train_attention_mask}\n",
    "test_inputs = {'input_ids': test_input_ids, 'attention_mask': test_attention_mask}\n",
    "\n",
    "# Create dataset class\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, label_masks):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.label_masks = label_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        item['label_masks'] = self.label_masks[idx]\n",
    "        return item\n",
    "\n",
    "# Create dataset objects for training and test sets\n",
    "train_dataset = MultiTaskDataset(train_inputs, train_labels, train_label_masks)\n",
    "test_dataset = MultiTaskDataset(test_inputs, test_labels, test_label_masks)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Use Cosine Annealing learning rate scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=32)\n",
    "\n",
    "# Improved Focal Loss implementation\n",
    "def focal_loss_with_dynamic_alpha(outputs, labels, label_masks, gamma=2):\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "    active_loss = labels != -1\n",
    "    active_loss = active_loss & (label_masks > 0)  # Apply mask\n",
    "\n",
    "    # Calculate the number of positive and negative samples\n",
    "    num_positive = torch.sum(labels[active_loss] == 1, dim=0).float()\n",
    "    num_negative = torch.sum(labels[active_loss] == 0, dim=0).float()\n",
    "\n",
    "    # Dynamically calculate α value, higher α for fewer positive samples\n",
    "    alpha = num_negative / (num_positive + num_negative + 1e-8)\n",
    "    \n",
    "    # Dynamic sample-level adjustment of alpha\n",
    "    alpha_factor = labels[active_loss].float() * alpha + (1 - labels[active_loss].float()) * (1 - alpha)\n",
    "\n",
    "    # Calculate basic cross-entropy loss\n",
    "    losses = loss_fn(outputs[active_loss], labels[active_loss].float())\n",
    "\n",
    "    # Calculate prediction probability p_t\n",
    "    probas = torch.sigmoid(outputs[active_loss])\n",
    "\n",
    "    # Adjust probability based on true labels\n",
    "    pt = probas * labels[active_loss].float() + (1 - probas) * (1 - labels[active_loss].float())\n",
    "\n",
    "    # Calculate Focal Loss term\n",
    "    focal_weight = (1 - pt) ** gamma\n",
    "\n",
    "    # Apply dynamically adjusted α value and focal weight\n",
    "    focal_loss = alpha_factor * focal_weight * losses\n",
    "\n",
    "    # Apply mask\n",
    "    masked_losses = focal_loss * label_masks[active_loss]\n",
    "\n",
    "    # Return average loss\n",
    "    return masked_losses.sum() / label_masks.sum()\n",
    "\n",
    "# Evaluation function, calculate loss, accuracy and AUC\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            label_masks = batch['label_masks']\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = focal_loss_with_dynamic_alpha(outputs, labels, label_masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()  # Get prediction probabilities\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    \n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(all_labels[all_labels != -1], binary_preds[all_labels != -1])\n",
    "\n",
    "    # Calculate AUC-ROC\n",
    "    auc = roc_auc_score(all_labels[all_labels != -1], all_preds[all_labels != -1])\n",
    "\n",
    "    return avg_loss, accuracy, auc\n",
    "\n",
    "# Train model and evaluate\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        label_masks = batch['label_masks']\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = focal_loss_with_dynamic_alpha(outputs, labels, label_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss}\")\n",
    "    \n",
    "    # Validation step\n",
    "    avg_val_loss, val_accuracy, val_auc = evaluate_model(model, test_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}, Accuracy: {val_accuracy}, AUC: {val_auc}\")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"saved_model\")\n",
    "tokenizer.save_pretrained(\"saved_model\")\n",
    "\n",
    "# Get chemical molecule representations and predicted values\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-1][:, 0, :]  # Get representation of [CLS] token\n",
    "    predictions = torch.sigmoid(outputs.logits)  # Get prediction probability for each label\n",
    "\n",
    "# Concatenate representations and predicted values\n",
    "enhanced_embeddings = torch.cat((embeddings, predictions), dim=1)\n",
    "\n",
    "print(\"Enhanced embeddings and predictions obtained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6e809-2612-4f15-b6b6-f7ca726261d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syq",
   "language": "python",
   "name": "syq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
